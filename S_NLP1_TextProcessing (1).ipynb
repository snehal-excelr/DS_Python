{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joXaY4CtrmmM"
      },
      "outputs": [],
      "source": [
        "# Inside nltk library you will get stop word, stemming, lemmatization. So install it\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "nltk.download('punkt') \n",
        "#punkt - This tokenizer divides a text into a list of sentences by using an unsupervised algorithm \n",
        "# to build a model for abbreviation words, collocations, and words that start sentences. \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#wordnet-WordNET is a lexical database of words in more than 200 languages in which we have adjectives, adverbs, nouns, \n",
        "# and verbs grouped differently into a set of cognitive synonyms, where each word in the database is expressing its distinct concept.\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "# PortStemmer - for stemming\n",
        "# WordNetLematizer - for lemmatization\n",
        "import re # for regular expression"
      ],
      "metadata": {
        "id": "QP8TYQ7lsNSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1efef8-783e-43b7-cbce-5c832698e143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text data\n",
        "para = \"\"\"‚Äî Tensor and Tensorflow: A powerful combo üí™\n",
        "The Google Brain team developed an advanced AI framework named Tensorflow years back. After that, Google designed its own processing unit named Tensor Processing Unit or TPU to perform more efficiently with the Tensorflow. The invention of TPU was a revolution in AI that has significantly expedited the training of huge machine learning models with millions (or, billions) of parameters. Nevertheless, that technology could not be used in low-power devices such as smartphones in Edge AI. The entrance of Google into the AI chip manufacturing club for low-power devices can be the next revolution in this industry. Many companies such as FogHorn and BlinkAI are working in Edge AI using currently existing AI chips in the market. However, the efficacy that Google can create by the combination of TensorFlow and Tensor will be game-changing. Welcome to the club, Google!\n",
        "\n",
        "‚Äî Tensor is an AI chip designed by AI! üò≤\n",
        "Isn‚Äôt that cool? The story is started from an article published in Nature titles ‚ÄúA graph placement methodology for fast chip design‚Äù. To design a processing chip, there is a crucial step referred to as ‚Äúfloor planning‚Äù where the engineering team must place a large number of components such that a series of physical requirements including power consumption and performance get satisfied. I don‚Äôt go further into its details as I am also not an expert in hardware engineering. However, when you have a large series of choices to make with a series of constraints AI can kick in. You may remember how the AlphaGo project defeated a professional human Go player. This is exactly the same. Tensor is the real outcome of this project that is a new milestone in the AI industry. Kudos, Google!\n",
        "\n",
        "‚Äî Tensor helps us build ethical AI. üí°\n",
        "This is a double-edged sword statement. Ethical AI has various aspects from data privacy to AI for all. Tensor helps many users have the opportunity to try the latest AI advancement while they have no concern about their privacy. Why? Because the AI engine is running on the chip, and no data is sent to the cloud for further computation. On the other hand, the more tightly Google binds AI software and hardware, the harder it will be for other companies to compete. I don‚Äôt want to see days that other companies can not even compete on performing AI inference, i.e., compete on using AI. We almost lost the game of model training to giant tech companies. It would be a nightmare if we lose the game on AI inference to them as well. That is why I believe ‚ÄúTensor helps us build ethical AI‚Äù is a double-edged sword.\"\"\""
      ],
      "metadata": {
        "id": "2ya3pgBusvqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para"
      ],
      "metadata": {
        "id": "ui49_rzGun_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jmva8a2xE0S",
        "outputId": "c51c3adb-cf6d-431d-ca71-1c54428e9717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2602"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in this paragraph special symbols, quotation marks, punchuation symbols, emojis are present\n",
        "# these are all unnecessary things.\n",
        "# Tokenization\n",
        "# Word Tokenization\n",
        "\n",
        "document = \"We are learning tokenization in NLP\"\n",
        "nltk.word_tokenize(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzTdZIXLupV6",
        "outputId": "54e8102c-459c-46d6-cfc4-e9c1b9cfc181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We', 'are', 'learning', 'tokenization', 'in', 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't do word tokenization directly on paragraph. \\n represents new line in paragraph. For that Sentence tokenization need to be done."
      ],
      "metadata": {
        "id": "R21HHLW_wgQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenization\n",
        "sent = nltk.sent_tokenize(para)"
      ],
      "metadata": {
        "id": "AlwkfvV5wM2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent) # length is 29 sentences i.e. 29 documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTs5l5iUxRii",
        "outputId": "84e439a7-40d1-43ce-a1dd-36ef4bafdeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent[0] # shows first document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kCVEkKQ6xUOh",
        "outputId": "75589ece-6df8-4049-cb85-3eaff16888c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'‚Äî Tensor and Tensorflow: A powerful combo üí™\\nThe Google Brain team developed an advanced AI framework named Tensorflow years back.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Cleaning - remove unnecessary thigs - punctuation marks, symbols, emojis, etc. using sub()\n",
        "# Text normalization - convert each word in lower case\n",
        "#  sub() returns a string where all matching occurrences of the specified pattern are replaced by the replace string.\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sent)):\n",
        "  txt = re.sub('[^a-zA-Z]',' ',sent[i])# except a-zA-Z remove everything from each sentence\n",
        "  txt = txt.lower()\n",
        "\n",
        "  corpus.append(txt)"
      ],
      "metadata": {
        "id": "7WcF0Z4yxkq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "X46V7CTQzZHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can perform stemming and lemmatization\n",
        "#Stemming\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "j7oM9eB4zali"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('goes')"
      ],
      "metadata": {
        "id": "CuXSa2pJzs66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('history')"
      ],
      "metadata": {
        "id": "aC9YB6div21m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('finally')"
      ],
      "metadata": {
        "id": "yxOMvLWtz5JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  print(words)\n",
        "  # we get separate list for each sentence"
      ],
      "metadata": {
        "id": "25WelZkhz7-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming and remove stop words also\n",
        "\n",
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i) # for each sentence in corpus perform word tokenization\n",
        "  for i in words: # for each unique value inside word\n",
        "    if i not in set(stopwords.words('english')): # will check words in stopwords from set of english stopwords\n",
        "      print(stemmer.stem(i)) # the words whch are not present in stopwords set print by performing stemming using stem() function.\n",
        "      # powerful - power, google-googl ... doen't make sense"
      ],
      "metadata": {
        "id": "NiYslxcB0h7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FF3-iiw1Xi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "tfNvuBlu0bpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('google')"
      ],
      "metadata": {
        "id": "PckAPFjq3GjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('historically')"
      ],
      "metadata": {
        "id": "3lg6oH5M3KS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('coming')"
      ],
      "metadata": {
        "id": "03PYxv1G3s9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for i in words:\n",
        "    if i not in set(stopwords.words('english')):\n",
        "      print(lemma.lemmatize(i)) \n",
        "      #proper google, powerful.. meaningful words are returned by lemmatization"
      ],
      "metadata": {
        "id": "tfc9_BWD3vHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction**"
      ],
      "metadata": {
        "id": "cKgt3gR64S4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text data to BoW (Bag of Words) or TFIDF (Term Frequency - Inverse Document Frequency)\n",
        "# CountVectorizer - for BoW, TfidfVectorizer - for TFIDF\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "HXnaCe-e4HG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer() # BoW - frequency will be displayed\n",
        "# Bag of Words (BoW) simply counts the frequency of words in a document.\n",
        "#cv = CountVectorizer(binary=True) # only binary weight will be displayed i.e. present or not\n",
        "x = cv.fit_transform(corpus)# \n",
        "cv.vocabulary_ # following words are taken as columns, no. represents index of each word\n"
      ],
      "metadata": {
        "id": "p-VCcUWU4l1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us see what type of BoW it has created. BoW - gives frequency of term in document, Binary Weight - tells word is present or not\n",
        "# We can't directly print matrix x. so we need to convert it to array\n",
        "x[0].toarray() # BoW for 1st sentence"
      ],
      "metadata": {
        "id": "wAtrx45p48nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0] # for this sentence above BoW is created. Why does it contain more values than the no. of words in this sentence?\n",
        "# Column contains all unique words."
      ],
      "metadata": {
        "id": "ydLIAv07GHLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e702b058-1cd9-489d-c72d-f23c8c5e6d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  tensor and tensorflow  a powerful combo   the google brain team developed an advanced ai framework named tensorflow years back '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert array into dataframe, DTM - Document Term Matrix\n",
        "x = pd.DataFrame(x.toarray(),columns=cv.get_feature_names_out())\n",
        "x\n",
        "# for 'advanced' word index was 1 so it is present at colummn index no. 1\n",
        "# in 5th document word 'ai' is occuring for 2 times.\n",
        "# BoW - gives you frequency\n",
        "# Binary weights - tells whether word is present or not\n",
        "# If you don't want to print frequency in dataframe, keep binary=True in 'cv = CountVectonizer(binary=True)'"
      ],
      "metadata": {
        "id": "6eKJYHccGK32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can start model building now. We have 216 columns i.e. 216 unique words are there. \n",
        "# 29 rows - 29 documents / sentences are there."
      ],
      "metadata": {
        "id": "WbqPtNmZMHh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF**\n",
        "Term frequency works by looking at the frequency of a particular term you are concerned with relative to the document. \n",
        "\n",
        "Inverse document frequency looks at how common (or uncommon) a word is amongst the corpus.\n",
        "\n"
      ],
      "metadata": {
        "id": "rNxMU2gsNO8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer()\n",
        "x = tf.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "jt9ZsrZ_G3QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in tf-idf we will see weightage not 0 and 1\n",
        "x.toarray()"
      ],
      "metadata": {
        "id": "wQDnJ6tANZwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert it into dataframe\n",
        "x = pd.DataFrame(x.toarray(),columns = tf.get_feature_names_out()) #get_feature_names_out() - gives you list of unique words inside corpus\n",
        "x # here we are retaining information related to the frequency of words, TDM-Term Document Frequency"
      ],
      "metadata": {
        "id": "EUuJ3rXqNmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddfXnW_uPREr",
        "outputId": "85d93437-a4b9-473f-b296-6613f94337ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, 216)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.get_feature_names_out()"
      ],
      "metadata": {
        "id": "fbyk3po0N6HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "gjBgrFlTNiYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c93c199-688b-4469-f364-73704a00608a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "216"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}
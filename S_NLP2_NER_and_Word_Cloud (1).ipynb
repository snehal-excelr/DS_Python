{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "HIUG4NaSA-S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "7DpFGx_hD_8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzpdKSPRvtut"
      },
      "outputs": [],
      "source": [
        "import spacy # contains model which we are going to use\n",
        "from spacy import displacy # to display along with tag\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('apple.txt',error_bad_lines=False) # to load txt file with csv keep error_bad_lines=Flase"
      ],
      "metadata": {
        "id": "21g2hKp7wWEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "zbjWzsLsw44l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to convert these sentences to paragraph for a corpus?"
      ],
      "metadata": {
        "id": "o7xxlq16w_mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x']# x is a column name"
      ],
      "metadata": {
        "id": "q_HXIiGbwwc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to join all these sentences."
      ],
      "metadata": {
        "id": "BMcZLD-8xgsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm') #model name is en_core_web_sm, nlp will now have entire NER model"
      ],
      "metadata": {
        "id": "UUcPmiulxsTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names # shows which different models are present inside en_core_web_sm model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVrS4Sobx4np",
        "outputId": "447a0917-3f4d-4c72-8907-1de8d68166f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we want ner (name entity recognition)\n",
        "\n",
        "doc = nlp('Mr.Elon is going to acquire Twitter Inc of $45 billion in 2023')"
      ],
      "metadata": {
        "id": "Gsumhsg1ybML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents# ents - will display all entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-qEkzMvyzsO",
        "outputId": "0c290654-0060-4aab-96ba-3cab5c96f161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Elon, Twitter Inc, $45 billion, 2023)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in doc.ents:\n",
        "  print(i, '|', i.label_)"
      ],
      "metadata": {
        "id": "ti7wFmGMy-Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp('Elon is going to acquire Twitter of $45 billion in 2023')"
      ],
      "metadata": {
        "id": "pj700rZdzS8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in doc1.ents:\n",
        "  print(i, '|', i.label_)"
      ],
      "metadata": {
        "id": "TEArStXr0_U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy library is not 100 % precise"
      ],
      "metadata": {
        "id": "vN0oPFjU1QHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to display graphical representation for tags\n",
        "displacy.render(doc,style='ent',jupyter=True) # ent - entity, In Jupyter instead of serve() try render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "LiMrSsRF1B4H",
        "outputId": "895d15a6-79d7-4a5f-b539-9a891feec254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Mr.\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Elon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is going to acquire \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Twitter Inc\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $45 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2023\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joining all sentences of dataset\n",
        "txt = ' '.join(df['x']) # with space join all sentences in x\n",
        "txt"
      ],
      "metadata": {
        "id": "69sTc2dD9UVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = nlp(txt)\n"
      ],
      "metadata": {
        "id": "b0J6Bnb5AfJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(x,style='ent',jupyter=True)"
      ],
      "metadata": {
        "id": "wUVFGpcqAsAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in x:\n",
        "  print(i,i.pos_)"
      ],
      "metadata": {
        "id": "GqOwIWllA2LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "syMaUJGlFbyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word cloud is a visualization technique for text data where the most frequent word is shown in the biggest font size.\n",
        "\n",
        "\n",
        "You can create Customized Word Cloud in python. Refer documentation"
      ],
      "metadata": {
        "id": "rI1xAg_vs1GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop = STOPWORDS"
      ],
      "metadata": {
        "id": "arAfbDRYG3Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc = WordCloud(background_color='white',stopwords=stop).generate(txt)\n",
        "plt.imshow(wc)\n",
        "# bigger the name of word more the frequency of that word in that corpus\n",
        "# font of apple, laptop is huge i.e. it is occuring many times in text\n",
        "# word cloud is important when we are analysing the data when we build Naive bias text classification.\n",
        "# This is how we can create word cloud"
      ],
      "metadata": {
        "id": "Q9obnqMnGpq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZFtp7tRG9nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnTJTXxvGzE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4vrZOoypAzq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to build an NLP pipeline\n",
        "\n",
        "Step1: Sentence Segmentation\n",
        "Sentence Segment is the first step for building the NLP pipeline. It breaks the paragraph into separate sentences.\n",
        "\n",
        "Step2: Word Tokenization\n",
        "Word Tokenizer is used to break the sentence into separate words or tokens.\n",
        "\n",
        "Step3: Stemming\n",
        "Stemming is used to normalize words into its base form or root form. For example, celebrates, celebrated and celebrating, all these words are originated with a single root word \"celebrate.\" The big problem with stemming is that sometimes it produces the root word which may not have any meaning.\n",
        "\n",
        "Step 4: Lemmatization\n",
        "Lemmatization is quite similar to the Stemming. It is used to group different inflected forms of the word, called Lemma. The main difference between Stemming and lemmatization is that it produces the root word, which has a meaning.\n",
        "\n",
        "Step 5: Identifying Stop Words\n",
        "In English, there are a lot of words that appear very frequently like \"is\", \"and\", \"the\", and \"a\". NLP pipelines will flag these words as stop words. Stop words might be filtered out before doing any statistical analysis.\n",
        "\n",
        "Step 6: Dependency Parsing\n",
        "Dependency Parsing is used to find that how all the words in the sentence are related to each other.\n",
        "\n",
        "Step 7: POS tags\n",
        "POS stands for parts of speech, which includes Noun, verb, adverb, and Adjective. It indicates that how a word functions with its meaning as well as grammatically within the sentences. A word has one or more parts of speech based on the context in which it is used.\n",
        "\n",
        "Step 8: Named Entity Recognition (NER)\n",
        "Named Entity Recognition (NER) is the process of detecting the named entity such as person name, movie name, organization name, or location.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ym_h1q0bxiom"
      }
    }
  ]
}